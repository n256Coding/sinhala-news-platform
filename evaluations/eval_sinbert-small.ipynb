{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[torch]\n",
    "# !pip install accelerate -U\n",
    "# !pip install pandas scikit-learn beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2senan77\\AppData\\Local\\anaconda3\\envs\\p3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import TrainingArguments\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from sinling import SinhalaTokenizer\n",
    "from sinling import SinhalaStemmer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlpc-uom/SinBERT-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords():\n",
    "    stopwords = []\n",
    "    with open('../dataset/stop words.txt', 'r', encoding='utf-8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "                \n",
    "            stopwords.append(line.strip())\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinhala_tokenizer = SinhalaTokenizer()\n",
    "stemmer = SinhalaStemmer()\n",
    "\n",
    "def preprocess(sentence: str):\n",
    "    \n",
    "    tokens = sinhala_tokenizer.tokenize(sentence)\n",
    "    stemmed = [stemmer.stem(token)[0] for token in tokens]\n",
    "    \n",
    "    return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ශ්‍ර ලංකා හ බංග්ලාදේශ අතර අ ඩකා පැවත්වෙ ආසියා ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>එල්.ටී.ටී.ඊ. සංවිධාන ත්‍රස්තවාද ක්‍රියාකාරක  ද...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>වසර 6  දොර වැඩකටයුත සඳහ අසතුට කාල වැයකර .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ඔ , 2008 වසර පැව පළ අදියර සි අනිද්ද ඇරඹෙ 8 ව අ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>බෙද ගියේ .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  labels\n",
       "0  ශ්‍ර ලංකා හ බංග්ලාදේශ අතර අ ඩකා පැවත්වෙ ආසියා ...       3\n",
       "1  එල්.ටී.ටී.ඊ. සංවිධාන ත්‍රස්තවාද ක්‍රියාකාරක  ද...       6\n",
       "2          වසර 6  දොර වැඩකටයුත සඳහ අසතුට කාල වැයකර .       0\n",
       "3  ඔ , 2008 වසර පැව පළ අදියර සි අනිද්ද ඇරඹෙ 8 ව අ...       3\n",
       "4                                         බෙද ගියේ .       3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comments', 'labels'],\n",
       "        num_rows: 5779\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comments', 'labels'],\n",
       "        num_rows: 723\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['comments', 'labels'],\n",
       "        num_rows: 722\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/upgraded-sinhala-news-categories.csv')\n",
    "dataset['comments'] = dataset['comments'].apply(preprocess)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "test_valid = dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset['validation'] = test_valid['train']\n",
    "dataset['test'] = test_valid['test']\n",
    "\n",
    "dataset_df = dataset['train'].to_pandas()\n",
    "display(dataset_df.head())\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comments': Value(dtype='string', id=None),\n",
       " 'labels': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = dataset['train'].features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'governmental_activities', \n",
    "    1: 'business', \n",
    "    2: 'technology', \n",
    "    3: 'sport', \n",
    "    4: 'entertainment', \n",
    "    5: 'international', \n",
    "    6: 'public_safety_incidents', \n",
    "    7: 'local', \n",
    "    8: 'health', \n",
    "    9: 'religion',\n",
    "    10: 'weather',\n",
    "    11: 'education', \n",
    "    12: 'security',\n",
    "    13: 'transport'\n",
    "}\n",
    "label2id = {v:k for k,v in id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2senan77\\AppData\\Local\\anaconda3\\envs\\p3.11\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 5779/5779 [00:00<00:00, 8623.10 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 723/723 [00:00<00:00, 8182.08 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 722/722 [00:00<00:00, 8775.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples['comments'], truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8562, 0.8252, 0.8801, 0.7749, 0.9365, 0.8259, 0.9578, 0.9685, 0.9936,\n",
       "        0.9979, 0.9945, 0.9934, 0.9988, 0.9967], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (1 - (dataset_df['labels'].value_counts().sort_index() / len(dataset_df))).values\n",
    "class_weights = torch.from_numpy(class_weights).float().to('cuda')\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nlpc-uom/SinBERT-small and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Feed inputs to model and extract logists\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Define loss function with class weights\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        # Compute loss\n",
    "        loss = loss_func(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=14,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2senan77\\AppData\\Local\\anaconda3\\envs\\p3.11\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "# Log the training loss at each epoch\n",
    "logging_steps = len(dataset[\"train\"]) // batch_size\n",
    "output_dir = \"sinbert-saved\"\n",
    "training_args = TrainingArguments(output_dir=output_dir,\n",
    "                                  num_train_epochs=10,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  fp16=True, # Make it train fast\n",
    "                                  push_to_hub=False\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedLossTrainer(model=model,\n",
    "                              args=training_args,\n",
    "                              compute_metrics=compute_metrics,\n",
    "                              train_dataset=dataset[\"train\"],\n",
    "                              eval_dataset=dataset[\"validation\"],\n",
    "                              tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1810/1810 18:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.218500</td>\n",
       "      <td>0.639845</td>\n",
       "      <td>0.804084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>0.838144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.481754</td>\n",
       "      <td>0.854392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.439619</td>\n",
       "      <td>0.864225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.429452</td>\n",
       "      <td>0.865154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.430903</td>\n",
       "      <td>0.876376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.443859</td>\n",
       "      <td>0.864795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.144500</td>\n",
       "      <td>0.453036</td>\n",
       "      <td>0.875353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.453787</td>\n",
       "      <td>0.868960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.458048</td>\n",
       "      <td>0.872169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1810, training_loss=0.365644266197036, metrics={'train_runtime': 1111.3278, 'train_samples_per_second': 52.001, 'train_steps_per_second': 1.629, 'total_flos': 7647547903324368.0, 'train_loss': 0.365644266197036, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
